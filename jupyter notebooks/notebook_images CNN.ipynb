{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9068897b-4d8a-4957-b4e0-208cc3012463",
    "_uuid": "3a84018dde0b34628526a96e52ad092d095610f7"
   },
   "source": [
    "Hello Kagglers. Working on Kaggle kernels is fun. The purpose of this kernel is totally different. Here I am not going to do a typical EDA or typical data modelling but I would love to share some cool things. We are going to dive into following topics:\n",
    "* How to add pre-trained Keras models to your kernel and answer the question **Why do I need to do that at all?**\n",
    "* What generator should I use for my model- inbuilt or a custom one?\n",
    "* How to effectively use Keras ImageDataGenerator in kernels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb1e0446-7d65-445e-a5eb-0df97ea9321e",
    "_uuid": "ec574a3d110b9a1dd77fa3617b90d6dba36066cc"
   },
   "source": [
    "## Adding Keras pre-trained models to your kernel\n",
    "\n",
    "Transfer learning (Here I am assuming that you know about it) **almost always** works. Before doing some serious modelling, people like me always starts with transfer learning to get a baseline. For this, we need pre-trained models. Keras provides a lot of SOTA pre-trained models. When you want to use a pre-trained architecture for the first time, Keras download the weights for the corresponding model *but* Kernels can't use network connection to download pretrained keras model weights. So, the big question is `If Kernels can't use network connection to download pre-trained weights, how can I use them at all?` \n",
    "\n",
    "This is a great question and for people who are beginners or just getting started on Kaggle kernels, this can be very confusing. In order to use, pre-trained Keras model weights, people have uploaded the weights to a kernel and published it. Now here is the catch. **You can add the output of any other kernel as input data source for your kernel **. Follow these simple steps:\n",
    "* On the top-left of your notebook, there is a `Input Files` cell. Expand it by clicking the `+` button.\n",
    "* You will see a list of input data files on the left along with the description of the data on the right.\n",
    "* Click the add `Add Data Source` button. A window will appear.\n",
    "* In the search bar, search like this `VGG16 pretrained` or `Keras-pretrained`.\n",
    "* Choose the kernel you want to add. That's it!!\n",
    "\n",
    "Now if you expand your `Input Files` cell again, you will the pre-trained model as input files along with your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e46bb37f-a8e6-4a79-b00f-652ceb1cd380",
    "_uuid": "b9df4b09af59e791d78c4211d624130c397339a8"
   },
   "source": [
    "You can see that my kernel has two kind of input files:\n",
    "* flowers-recognition dataset\n",
    "* vgg16 pre-trained model kernel that I added to my kernel\n",
    "\n",
    "Keras requires the pre-trained weights to be present in the `.keras/models` cache directory. This is how you do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9a068efc-eefa-46c2-a824-cf9f78f6e438",
    "_uuid": "c26f5ee1e57e052b0ab4882b0f7c1c5df226afba"
   },
   "source": [
    "That's it!! Now, you can use pre-trained models for transfer learning or fine-tuning. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bebb498c-6dbf-4dac-ba65-7f8d628e5d40",
    "_uuid": "2eb0d1f2ecb6751bad1d551ce6595bfeecca71e7"
   },
   "source": [
    "**What generator should I use for my model-  a custom one or the default Keras ImageDataGenerator?**\n",
    "\n",
    "This is a very interesting question. I would say that it actually depends on how your dataset is arranged or how are you going to set up your data. These are the following scenarios I can think of along with the corresponding solutions. If you think of any more, do let me know in the comments section.\n",
    "\n",
    "* **Data is arranged class-wise in separate directories with corresponding names**: This is the best way to arrange your data, if possible. Although it takes some time to arrange the data in such a way but it is the way to go if you want to use the Keras ImageDataGenerator efficiently as it requires data to be separated class wise in different folders. Once you have this, you need to arrange your data like this:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        category1/(contains all images related to category1)  \n",
    "        category2/(contains all images related to category2)\n",
    "        ...\n",
    "        ...\n",
    "            \n",
    "    validation/\n",
    "         category1/(contains all images related to category1)  \n",
    "        category2/(contains all images related to category2)\n",
    "        ...\n",
    "        ...\n",
    "```\n",
    "For this kernel, later in the notebook, I will show how to make this structure within the kernel for using ImageDataGenerator\n",
    "\n",
    "* **All data is within one folder and you have meta info about the images** This is a very usual case. When we quickly crawl data, we generally store the met info about the images in a csv and allthe images are stored in a single folder. There are two ways to deal with this situatio, provided you don't want all the segregation of images as in the first step.\n",
    "  * Define your own simple python generator which yields batches of images and labels while reading the csv\n",
    "  * Use another high-level api such as `Dataset` api and let it do the work for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f5ff66b-d076-4392-a74d-5c1dd9ffafb5",
    "_uuid": "70fa89bda45f2f850df220bc219aaf042bebab80"
   },
   "source": [
    "Let's look at how to get the structure defined in the  first step above. If you are not aware, jupyter is pretty powerful and you can use bash directly within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b75909bd-1584-41c0-ace0-810198f522f6",
    "_uuid": "7b3e162a7b9ee182ab7e9d47a569261153d56dd1"
   },
   "source": [
    "For each category, copy samples to the train and validation directory which we defined in the above step. The number of samples you want in your training and validation set is upto you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b27904f4-a596-4b12-a456-2ccd11998ccf",
    "_uuid": "3d481ccb412779b5aed48df2cb39888fa07579da"
   },
   "source": [
    "That's all folks. I hope you enjoyed this. One last thing: Kaggle kernels doesn't provide you GPU, so the training time will depend on your architecture and size of your dataset. Also, if you find this kernel helpful, please upvote!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "51c87806-9ade-4ebe-b108-0baa4af062ad",
    "_uuid": "ce6a882ae0c2a4363143831849153dd70442ca6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "622a2872-cbfa-493a-96d3-b392afe30cd0",
    "_uuid": "1916ec613e5a35b17cf3fb35e98e2180d84921d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../input/furniture/data/data/train/'\n",
    "valid_path = '../input/furniture/data/data/valid/'\n",
    "test_path = '../input/furnituretest/test/test/unknown/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "f877e4a5-937b-46b4-9aa4-546be49e6d8f",
    "_uuid": "50cd83f32c9290aedcfe6260956dacaf182f693f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 972 images belonging to 5 classes.\n",
      "Found 500 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator().flow_from_directory(train_path,target_size=(224,224),classes=['104','15','5','67','86'],batch_size=50)\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,target_size=(224,224),classes=['104','15','5','67','86'],batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "2c3e0ec1-52d4-49e1-94b1-d37bd5d1b46f",
    "_uuid": "9ebbff29ed749300fc0e25dc95d6225001c5bb82",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),Flatten(),Dense(5,activation='softmax'),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a78cdd68-aa75-4074-9a13-23ae35e9b073",
    "_uuid": "ba71a89404b7992d773ef6aa71096b2a62be9614",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "6d6399ec-4fa0-4e05-a994-11a84695b6e0",
    "_uuid": "7be0b370170047d143e05fee1dbbca7ba857ae53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 27s 1s/step - loss: 12.9343 - acc: 0.1955 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 14s 721ms/step - loss: 12.8314 - acc: 0.2039 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 764ms/step - loss: 12.7924 - acc: 0.2063 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 15s 765ms/step - loss: 12.7728 - acc: 0.2075 - val_loss: 12.8945 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 15s 750ms/step - loss: 12.7533 - acc: 0.2088 - val_loss: 12.8945 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b254eac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches,validation_data=valid_batches,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "808fa653-ffe4-4a96-a653-d38ac2848b44",
    "_uuid": "d8fc23e19aeaac6fb816b66c642cbb264090e49a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preditions = model.predict_generator(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "2811fc3e-4383-4948-b3a3-eeb3d4c38294",
    "_uuid": "eab25ddae24235b8a082c14d59bd0ed018828017"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs,vlabels = next(valid_batches)\n",
    "len(vlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6a31f3c8-94e4-4f97-8091-fdf3385888c8",
    "_uuid": "a20681a0299768f76c804f6d7d5ddf0fe14c2002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "585abffe-78cd-4cb9-bb2d-e1ea34ef740d",
    "_uuid": "c7efe160e6e1a5174e2c9b81e2a21c807766e401"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.894476509094238, 0.20000000074505805]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "d09d8129-3b9f-4a91-8aed-2f2fae19f806",
    "_uuid": "c9d7ffa56db2d5b079106ac0e0be6c627676f587"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a0b4e6d70d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreditions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "# cm = confusion_matrix(vlabels,preditions[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "56ea282c-184c-443f-9542-c549edfad514",
    "_uuid": "b452b8a0b1be85cfa4122b48d0f037e4d02b9af0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vggmodel = keras.applications.vgg16.VGG16()\n",
    "# # vgg = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "# vggmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "4b94d6cc-2642-416b-b39d-2a19f8e5c485",
    "_uuid": "8bae96aa0f5f347b566f8d7e83422f95b5fac59e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "dbbd9007-4845-4c7e-ac5d-6259147c53f5",
    "_uuid": "79d1ab42174acbe6955760bf49212583baa7f2d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(224, 224,...)`\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(224, 224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "9d68d39d-b661-4d4a-babd-3f2d4815f66f",
    "_uuid": "653db3b3e59aac5e3b6abbf678fade3e0ec301d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "c8e88202-0679-4246-b277-ce998d2a00d9",
    "_uuid": "1e90837685cb2a7fbd2e650475fefde20836fb0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 20s 998ms/step - loss: 2.4121 - acc: 0.2033 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 15s 759ms/step - loss: 1.8489 - acc: 0.2166 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 761ms/step - loss: 1.7816 - acc: 0.2055 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 15s 763ms/step - loss: 1.8845 - acc: 0.2085 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 15s 756ms/step - loss: 1.7571 - acc: 0.2013 - val_loss: 1.6094 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b1ddfef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches,validation_data=valid_batches,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "46c3ee5f-e86b-49a1-ac0e-7bba02636cb5",
    "_uuid": "7d813daf94f416ad26e1f1f59f75275d5771ea88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1.8273 - acc: 0.1997 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 15s 742ms/step - loss: 1.8027 - acc: 0.2037 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 758ms/step - loss: 1.7157 - acc: 0.2027 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 15s 756ms/step - loss: 1.7990 - acc: 0.2049 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 15s 747ms/step - loss: 1.7241 - acc: 0.2049 - val_loss: 1.6094 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b1ddfe10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches,validation_data=valid_batches,epochs=5,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "76041fbd-d8cb-4c84-bf73-fa5463ee2c2e",
    "_uuid": "76986e85d0e77990846adb1e72f858657d1acc8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 972 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen_augmented = ImageDataGenerator(\n",
    "        rescale=1./255,        # normalize pixel values to [0,1]\n",
    "        shear_range=0.2,       # randomly applies shearing transformation\n",
    "        zoom_range=0.2,        # randomly applies shearing transformation\n",
    "        horizontal_flip=True)  # randomly flip the images\n",
    "\n",
    "# same code as before\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(train_path,target_size=(224,224),classes=['104','15','5','67','86'],batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "3fe62989-29f0-4cff-8722-cfbaa00e7b60",
    "_uuid": "2d01366d6279fb34e8d182545b045bd56d6a66ac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.00001,decay=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "74185d9c-c6d9-4784-9c8a-d680c33e5a1e",
    "_uuid": "4f956735f46108b78c97c3ca04c20560ec010025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 31s 2s/step - loss: 1.6152 - acc: 0.1889 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.6058 - acc: 0.2174 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.6102 - acc: 0.2120 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.6069 - acc: 0.2254 - val_loss: 1.6094 - val_acc: 0.2000\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.6057 - acc: 0.2191 - val_loss: 1.6094 - val_acc: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b04eeba8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator_augmented,validation_data=valid_batches,epochs=5,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "6ce25b12-b87d-453b-bee4-7346d9651903",
    "_uuid": "b021962176815cfa9fb3224c5496ce5f9dd56373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(224, 224,...)`\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(224, 224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "f1a92ca0-6018-4be7-bc1b-e362e5c6254a",
    "_uuid": "8977297c338d2e496179a4144120e4d481973af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 31s 2s/step - loss: 1.6048 - acc: 0.2216 - val_loss: 8.5381 - val_acc: 0.3320\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.5846 - acc: 0.2670 - val_loss: 8.6453 - val_acc: 0.3440\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.5733 - acc: 0.2877 - val_loss: 9.1492 - val_acc: 0.3380\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.5520 - acc: 0.3374 - val_loss: 8.2626 - val_acc: 0.3820\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.5407 - acc: 0.3317 - val_loss: 9.3986 - val_acc: 0.3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b16502b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(lr=0.00001,decay=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator_augmented,validation_data=valid_batches,epochs=5,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "7ddf5604-8bdd-4190-953f-ce751033db77",
    "_uuid": "7ed5fa94de382727b11329c046ed4d787b005830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 31s 2s/step - loss: 1.5318 - acc: 0.3452 - val_loss: 7.6075 - val_acc: 0.4440\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.5199 - acc: 0.3532 - val_loss: 8.1381 - val_acc: 0.4180\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4926 - acc: 0.3924 - val_loss: 7.6518 - val_acc: 0.4680\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4793 - acc: 0.3952 - val_loss: 8.0083 - val_acc: 0.4560\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4641 - acc: 0.3997 - val_loss: 7.6456 - val_acc: 0.4640\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4518 - acc: 0.3658 - val_loss: 7.1999 - val_acc: 0.5100\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4449 - acc: 0.4187 - val_loss: 7.5405 - val_acc: 0.4840\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4246 - acc: 0.3936 - val_loss: 8.0333 - val_acc: 0.4560\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4034 - acc: 0.4329 - val_loss: 8.0208 - val_acc: 0.4640\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.4054 - acc: 0.4211 - val_loss: 7.5180 - val_acc: 0.4940\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3893 - acc: 0.4131 - val_loss: 7.2248 - val_acc: 0.5160\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3726 - acc: 0.4495 - val_loss: 7.8509 - val_acc: 0.4860\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3745 - acc: 0.4461 - val_loss: 7.3940 - val_acc: 0.4980\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3563 - acc: 0.4539 - val_loss: 6.9499 - val_acc: 0.5280\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3439 - acc: 0.4409 - val_loss: 7.1498 - val_acc: 0.5280\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3434 - acc: 0.4569 - val_loss: 7.1036 - val_acc: 0.5400\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3190 - acc: 0.4811 - val_loss: 7.1027 - val_acc: 0.5220\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.3232 - acc: 0.4679 - val_loss: 6.9378 - val_acc: 0.5440\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2928 - acc: 0.4642 - val_loss: 6.5399 - val_acc: 0.5700\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2774 - acc: 0.4888 - val_loss: 7.0621 - val_acc: 0.5400\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2701 - acc: 0.4972 - val_loss: 6.6114 - val_acc: 0.5720\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2937 - acc: 0.4724 - val_loss: 7.0589 - val_acc: 0.5460\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2668 - acc: 0.4872 - val_loss: 6.6511 - val_acc: 0.5620\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2661 - acc: 0.5143 - val_loss: 6.8375 - val_acc: 0.5560\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2510 - acc: 0.4984 - val_loss: 6.8931 - val_acc: 0.5540\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2660 - acc: 0.5112 - val_loss: 6.2395 - val_acc: 0.5960\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2443 - acc: 0.5018 - val_loss: 6.3343 - val_acc: 0.5960\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2113 - acc: 0.5275 - val_loss: 6.6043 - val_acc: 0.5720\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2298 - acc: 0.5016 - val_loss: 6.1833 - val_acc: 0.5920\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 26s 1s/step - loss: 1.2128 - acc: 0.5192 - val_loss: 6.6466 - val_acc: 0.5720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68b0f8b9e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(Adam(lr=0.00001,decay=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator_augmented,validation_data=valid_batches,epochs=30,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "533e3240eb726b593642dd883ae5e8896b2673f9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = '../input/furnituretest/test/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "5e7960779589bd07df878616bd07838929dd40a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# train_batches = ImageDataGenerator().flow_from_directory(train_path,target_size=(224,224),classes=['104','15','5','67','86'],batch_size=50)\n",
    "# valid_batches = ImageDataGenerator().flow_from_directory(valid_path,target_size=(224,224),classes=['104','15','5','67','86'],batch_size=50)\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path,target_size=(224,224),classes=None,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "a33dda00-0c69-4891-b3c0-fb19a62db8b4",
    "_uuid": "f092744ceb1f969ba691922fd853aa8f0fba9fa6"
   },
   "outputs": [],
   "source": [
    "preditions = model.predict_generator(test_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "3d547693fb4026bc321982e967e1df57ded49b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 5.0214800e-28 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 1.0940081e-29 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.5649786e-14 0.0000000e+00]\n",
      " [4.6142077e-08 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.2430840e-20 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 5.6755841e-01 4.3244159e-01 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [9.9999845e-01 1.5055867e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 2.3495356e-37 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [3.3409903e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [5.6703093e-06 9.9999428e-01 0.0000000e+00 1.0264415e-08 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.4066793e-01 8.5933208e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 4.7314298e-04 0.0000000e+00 9.9952686e-01 0.0000000e+00]\n",
      " [7.7310508e-10 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 3.9070974e-19 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [2.3400735e-05 9.9997663e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [9.9985969e-01 1.4029646e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [6.7803699e-23 1.3149651e-25 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
      " [2.6111671e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [7.1186054e-29 1.0000000e+00 0.0000000e+00 1.5067708e-31 0.0000000e+00]\n",
      " [2.8976409e-21 1.7384750e-09 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [1.1485862e-13 1.0000000e+00 0.0000000e+00 5.8687394e-19 2.8971532e-37]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 7.1168786e-11 1.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(preditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "195663678d449aab114399acf50f13020ac43c5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0.0000000e+00 0.0000000e+00 5.0214800e-28 0.0000000e+00 1.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 1.0000000e+00 1.0940081e-29 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.5649786e-14 0.0000000e+00]\n",
    " [4.6142077e-08 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 1.2430840e-20 0.0000000e+00 1.0000000e+00]\n",
    " [0.0000000e+00 5.6755841e-01 4.3244159e-01 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [9.9999845e-01 1.5055867e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 2.3495356e-37 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [3.3409903e-32 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [5.6703093e-06 9.9999428e-01 0.0000000e+00 1.0264415e-08 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [1.4066793e-01 8.5933208e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 4.7314298e-04 0.0000000e+00 9.9952686e-01 0.0000000e+00]\n",
    " [7.7310508e-10 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 3.9070974e-19 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [2.3400735e-05 9.9997663e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [9.9985969e-01 1.4029646e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [6.7803699e-23 1.3149651e-25 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
    " [2.6111671e-29 1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
    " [7.1186054e-29 1.0000000e+00 0.0000000e+00 1.5067708e-31 0.0000000e+00]\n",
    " [2.8976409e-21 1.7384750e-09 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
    " [1.1485862e-13 1.0000000e+00 0.0000000e+00 5.8687394e-19 2.8971532e-37]\n",
    " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00]\n",
    " [0.0000000e+00 0.0000000e+00 7.1168786e-11 1.0000000e+00 0.0000000e+00]\n",
    " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaa0f467563f72e6f9550e88cfa735399fbee616",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
